Provided proper attribution is provided, Google hereby grants permission to
reproduce the tables and figures in this paper solely for use in journalistic or
scholarly works.
Attention Is All You Need
Ashish Vaswani∗
Google Brain
avaswani@google.com
Noam Shazeer∗
Google Brain
noam@google.com
Niki Parmar∗
Google Research
nikip@google.com
Jakob Uszkoreit∗
Google Research
usz@google.com
Llion Jones∗
Google Research
llion@google.com
Aidan N. Gomez∗ †
University of Toronto
aidan@cs.toronto.edu
Łukasz Kaiser∗
Google Brain
lukaszkaiser@google.com
Illia Polosukhin∗ ‡
illia.polosukhin@gmail.com
Abstract
The dominant sequence transduction models are based on complex recurrent or
convolutional neural networks that include an encoder and a decoder. The best
performing models also connect the encoder and decoder through an attention
mechanism. We propose a new simple network architecture, the Transformer,
based solely on attention mechanisms, dispensing with recurrence and convolutions
entirely. Experiments on two machine translation tasks show these models to
be superior in quality while being more parallelizable and requiring significantly
less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including
ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,
our model establishes a new single-model state-of-the-art BLEU score of 41.8 after
training for 3.5 days on eight GPUs, a small fraction of the training costs of the
best models from the literature. We show that the Transformer generalizes well to
other tasks by applying it successfully to English constituency parsing both with
large and limited training data.
∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started
the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and
has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head
attention and the parameter-free position representation and became the other person involved in nearly every
detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and
tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and
efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and
implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating
our research.
†Work performed while at Google Brain.
‡Work performed while at Google Research.
31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.
arXiv:1706.03762v7 [cs.CL] 2 Aug 2023
1 Introduction
Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks
in particular, have been firmly established as state of the art approaches in sequence modeling and
transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous
efforts have since continued to push the boundaries of recurrent language models and encoder-decoder
architectures [38, 24, 15].
Recurrent models typically factor computation along the symbol positions of the input and output
sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden
states ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently
sequential nature precludes parallelization within training examples, which becomes critical at longer
sequence lengths, as memory constraints limit batching across examples. Recent work has achieved
significant improvements in computational efficiency through factorization tricks [21] and conditional
computation [32], while also improving model performance in case of the latter. The fundamental
constraint of sequential computation, however, remains.
Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in
the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms
are used in conjunction with a recurrent network.
In this work we propose the Transformer, a model architecture eschewing recurrence and instead
relying entirely on an attention mechanism to draw global dependencies between input and output.
The Transformer allows for significantly more parallelization and can reach a new state of the art in
translation quality after being trained for as little as twelve hours on eight P100 GPUs.
2 Background
The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU
[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building
block, computing hidden representations in parallel for all input and output positions. In these models,
the number of operations required to relate signals from two arbitrary input or output positions grows
in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes
it more difficult to learn dependencies between distant positions [12]. In the Transformer this is
reduced to a constant number of operations, albeit at the cost of reduced effective resolution due
to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as
described in section 3.2.
Self-attention, sometimes called intra-attention is an attention mechanism relating different positions
of a single sequence in order to compute a representation of the sequence. Self-attention has been
used successfully in a variety of tasks including reading comprehension, abstractive summarization,
textual entailment and learning task-independent sentence representations [4, 27, 28, 22].
End-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and
language modeling tasks [34].
To the best of our knowledge, however, the Transformer is the first transduction model relying
entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate
self-attention and discuss its advantages over models such as [17, 18] and [9].
3 Model Architecture
Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].
Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence
of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output
sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive
[10], consuming the previously generated symbols as additional input when generating the next.
2
Figure 1: The Transformer - model architecture.
The Transformer follows this overall architecture using stacked self-attention and point-wise, fully
connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,
respectively.
3.1 Encoder and Decoder Stacks
Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two
sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network. We employ a residual connection [11] around each of
the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is
LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer
itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension dmodel = 512.
Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two
sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head
attention over the output of the encoder stack. Similar to the encoder, we employ residual connections
around each of the sub-layers, followed by layer normalization. We also modify the self-attention
sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This
masking, combined with fact that the output embeddings are offset by one position, ensures that the
predictions for position i can depend only on the known outputs at positions less than i.
3.2 Attention
An attention function can be described as mapping a query and a set of key-value pairs to an output,
where the query, keys, values, and output are all vectors. The output is computed as a weighted sum
3
Scaled Dot-Product Attention Multi-Head Attention
Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several
attention layers running in parallel.
of the values, where the weight assigned to each value is computed by a compatibility function of the
query with the corresponding key.
3.2.1 Scaled Dot-Product Attention
We call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists of
queries and keys of dimension dk, and values of dimension dv. We compute the dot products of the
query with all keys, divide each by √
dk, and apply a softmax function to obtain the weights on the
values.
In practice, we compute the attention function on a set of queries simultaneously, packed together
into a matrix Q. The keys and values are also packed together into matrices K and V . We compute
the matrix of outputs as:
Attention(Q, K, V ) = softmax(QKT
√
dk
)V (1)
The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor
of √
1
dk
. Additive attention computes the compatibility function using a feed-forward network with
a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is
much faster and more space-efficient in practice, since it can be implemented using highly optimized
matrix multiplication code.
While for small values of dk the two mechanisms perform similarly, additive attention outperforms
dot product attention without scaling for larger values of dk [3]. We suspect that for large values of
dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has
extremely small gradients 4
. To counteract this effect, we scale the dot products by √
1
dk
.
3.2.2 Multi-Head Attention
Instead of performing a single attention function with dmodel-dimensional keys, values and queries,
we found it beneficial to linearly project the queries, keys and values h times with different, learned
linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of
queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional
4To illustrate why the dot products get large, assume that the components of q and k are independent random
variables with mean 0 and variance 1. Then their dot product, q · k =
Pdk
i=1 qiki, has mean 0 and variance dk.
4
output values. These are concatenated and once again projected, resulting in the final values, as
depicted in Figure 2.
Multi-head attention allows the model to jointly attend to information from different representation
subspaces at different positions. With a single attention head, averaging inhibits this.
MultiHead(Q, K, V ) = Concat(head1, ..., headh)WO
where headi = Attention(QWQ
i
, KW K
i
, V WV
i
)
Where the projections are parameter matrices W
Q
i ∈ R
dmodel×dk , W K
i ∈ R
dmodel×dk , WV
i ∈ R
dmodel×dv
and WO ∈ R
hdv×dmodel
.
In this work we employ h = 8 parallel attention layers, or heads. For each of these we use
dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost
is similar to that of single-head attention with full dimensionality.
3.2.3 Applications of Attention in our Model
The Transformer uses multi-head attention in three different ways:
• In "encoder-decoder attention" layers, the queries come from the previous decoder layer,
and the memory keys and values come from the output of the encoder. This allows every
position in the decoder to attend over all positions in the input sequence. This mimics the
typical encoder-decoder attention mechanisms in sequence-to-sequence models such as
[38, 2, 9].
• The encoder contains self-attention layers. In a self-attention layer all of the keys, values
and queries come from the same place, in this case, the output of the previous layer in the
encoder. Each position in the encoder can attend to all positions in the previous layer of the
encoder.
• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to
all positions in the decoder up to and including that position. We need to prevent leftward
information flow in the decoder to preserve the auto-regressive property. We implement this
inside of scaled dot-product attention by masking out (setting to −∞) all values in the input
of the softmax which correspond to illegal connections. See Figure 2.
3.3 Position-wise Feed-Forward Networks
In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully
connected feed-forward network, which is applied to each position separately and identically. This
consists of two linear transformations with a ReLU activation in between.
FFN(x) = max(0, xW1 + b1)W2 + b2 (2)
While the linear transformations are the same across different positions, they use different parameters
from layer to layer. Another way of describing this is as two convolutions with kernel size 1.
The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality
df f = 2048.
3.4 Embeddings and Softmax
Similarly to other sequence transduction models, we use learned embeddings to convert the input
tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In
our model, we share the same weight matrix between the two embedding layers and the pre-softmax
linear transformation, similar to [30]. In the embedding layers, we multiply those weights by √
dmodel.
5
Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations
for different layer types. n is the sequence length, d is the representation dimension, k is the kernel
size of convolutions and r the size of the neighborhood in restricted self-attention.
Layer Type Complexity per Layer Sequential Maximum Path Length
Operations
Self-Attention O(n
2
· d) O(1) O(1)
Recurrent O(n · d
2
) O(n) O(n)
Convolutional O(k · n · d
2
) O(1) O(logk(n))
Self-Attention (restricted) O(r · n · d) O(1) O(n/r)
3.5 Positional Encoding
Since our model contains no recurrence and no convolution, in order for the model to make use of the
order of the sequence, we must inject some information about the relative or absolute position of the
tokens in the sequence. To this end, we add "positional encodings" to the input embeddings at the
bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel
as the embeddings, so that the two can be summed. There are many choices of positional encodings,
learned and fixed [9].
In this work, we use sine and cosine functions of different frequencies:
P E(pos,2i) = sin(pos/100002i/dmodel)
P E(pos,2i+1) = cos(pos/100002i/dmodel)
where pos is the position and i is the dimension. That is, each dimension of the positional encoding
corresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We
chose this function because we hypothesized it would allow the model to easily learn to attend by
relative positions, since for any fixed offset k, P Epos+k can be represented as a linear function of
P Epos.
We also experimented with using learned positional embeddings [9] instead, and found that the two
versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version
because it may allow the model to extrapolate to sequence lengths longer than the ones encountered
during training.
4 Why Self-Attention
In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations
(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi
, zi ∈ R
d
, such as a hidden
layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we
consider three desiderata.
One is the total computational complexity per layer. Another is the amount of computation that can
be parallelized, as measured by the minimum number of sequential operations required.
The third is the path length between long-range dependencies in the network. Learning long-range
dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the
ability to learn such dependencies is the length of the paths forward and backward signals have to
traverse in the network. The shorter these paths between any combination of positions in the input
and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare
the maximum path length between any two input and output positions in networks composed of the
different layer types.
As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially
executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of
computational complexity, self-attention layers are faster than recurrent layers when the sequence
6
length n is smaller than the representation dimensionality d, which is most often the case with
sentence representations used by state-of-the-art models in machine translations, such as word-piece
[38] and byte-pair [31] representations. To improve computational performance for tasks involving
very long sequences, self-attention could be restricted to considering only a neighborhood of size r in
the input sequence centered around the respective output position. This would increase the maximum
path length to O(n/r). We plan to investigate this approach further in future work.
A single convolutional layer with kernel width k < n does not connect all pairs of input and output
positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,
or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths
between any two positions in the network. Convolutional layers are generally more expensive than
recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity
considerably, to O(k · n · d + n · d
2
). Even with k = n, however, the complexity of a separable
convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,
the approach we take in our model.
As side benefit, self-attention could yield more interpretable models. We inspect attention distributions
from our models and present and discuss examples in the appendix. Not only do individual attention
heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic
and semantic structure of the sentences.
5 Training
This section describes the training regime for our models.
5.1 Training Data and Batching
We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million
sentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT
2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece
vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training
batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000
target tokens.
5.2 Hardware and Schedule
We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using
the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We
trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the
bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps
(3.5 days).
5.3 Optimizer
We used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9
. We varied the learning
rate over the course of training, according to the formula:
lrate = d
−0.5
model · min(step_num−0.5
, step_num · warmup_steps−1.5
) (3)
This corresponds to increasing the learning rate linearly for the first warmup_steps training steps,
and decreasing it thereafter proportionally to the inverse square root of the step number. We used
warmup_steps = 4000.
5.4 Regularization
We employ three types of regularization during training:
7
Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the
English-to-German and English-to-French newstest2014 tests at a fraction of the training cost.
Model
BLEU Training Cost (FLOPs)
EN-DE EN-FR EN-DE EN-FR
ByteNet [18] 23.75
Deep-Att + PosUnk [39] 39.2 1.0 · 1020
GNMT + RL [38] 24.6 39.92 2.3 · 1019 1.4 · 1020
ConvS2S [9] 25.16 40.46 9.6 · 1018 1.5 · 1020
MoE [32] 26.03 40.56 2.0 · 1019 1.2 · 1020
Deep-Att + PosUnk Ensemble [39] 40.4 8.0 · 1020
GNMT + RL Ensemble [38] 26.30 41.16 1.8 · 1020 1.1 · 1021
ConvS2S Ensemble [9] 26.36 41.29 7.7 · 1019 1.2 · 1021
Transformer (base model) 27.3 38.1 3.3 · 1018
Transformer (big) 28.4 41.8 2.3 · 1019
Residual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the
sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the
positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of
Pdrop = 0.1.
Label Smoothing During training, we employed label smoothing of value ϵls = 0.1 [36]. This
hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.
6 Results
6.1 Machine Translation
On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)
in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0
BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is
listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model
surpasses all previously published models and ensembles, at a fraction of the training cost of any of
the competitive models.
On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,
outperforming all of the previously published single models, at less than 1/4 the training cost of the
previous state-of-the-art model. The Transformer (big) model trained for English-to-French used
dropout rate Pdrop = 0.1, instead of 0.3.
For the base models, we used a single model obtained by averaging the last 5 checkpoints, which
were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We
used beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters
were chosen after experimentation on the development set. We set the maximum output length during
inference to input length + 50, but terminate early when possible [38].
Table 2 summarizes our results and compares our translation quality and training costs to other model
architectures from the literature. We estimate the number of floating point operations used to train a
model by multiplying the training time, the number of GPUs used, and an estimate of the sustained
single-precision floating-point capacity of each GPU 5
.
6.2 Model Variations
To evaluate the importance of different components of the Transformer, we varied our base model
in different ways, measuring the change in performance on English-to-German translation on the
5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.
8
Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base
model. All metrics are on the English-to-German translation development set, newstest2013. Listed
perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to
per-word perplexities.
N dmodel dff h dk dv Pdrop ϵls
train PPL BLEU params
steps (dev) (dev) ×106
base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65
(A)
1 512 512 5.29 24.9
4 128 128 5.00 25.5
16 32 32 4.91 25.8
32 16 16 5.01 25.4
(B) 16 5.16 25.1 58
32 5.01 25.4 60
(C)
2 6.11 23.7 36
4 5.19 25.3 50
8 4.88 25.5 80
256 32 32 5.75 24.5 28
1024 128 128 4.66 26.0 168
1024 5.12 25.4 53
4096 4.75 26.2 90
(D)
0.0 5.77 24.6
0.2 4.95 25.5
0.0 4.67 25.3
0.2 5.47 25.7
(E) positional embedding instead of sinusoids 4.92 25.7
big 6 1024 4096 16 0.3 300K 4.33 26.4 213
development set, newstest2013. We used beam search as described in the previous section, but no
checkpoint averaging. We present these results in Table 3.
In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,
keeping the amount of computation constant, as described in Section 3.2.2. While single-head
attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.
In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This
suggests that determining compatibility is not easy and that a more sophisticated compatibility
function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,
bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our
sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical
results to the base model.
6.3 English Constituency Parsing
To evaluate if the Transformer can generalize to other tasks we performed experiments on English
constituency parsing. This task presents specific challenges: the output is subject to strong structural
constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence
models have not been able to attain state-of-the-art results in small-data regimes [37].
We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the
Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,
using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences
[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens
for the semi-supervised setting.
We performed only a small number of experiments to select the dropout, both attention and residual
(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters
remained unchanged from the English-to-German base translation model. During inference, we
9
Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23
of WSJ)
Parser Training WSJ 23 F1
Vinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3
Petrov et al. (2006) [29] WSJ only, discriminative 90.4
Zhu et al. (2013) [40] WSJ only, discriminative 90.4
Dyer et al. (2016) [8] WSJ only, discriminative 91.7
Transformer (4 layers) WSJ only, discriminative 91.3
Zhu et al. (2013) [40] semi-supervised 91.3
Huang & Harper (2009) [14] semi-supervised 91.3
McClosky et al. (2006) [26] semi-supervised 92.1
Vinyals & Kaiser el al. (2014) [37] semi-supervised 92.1
Transformer (4 layers) semi-supervised 92.7
Luong et al. (2015) [23] multi-task 93.0
Dyer et al. (2016) [8] generative 93.3
increased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3
for both WSJ only and the semi-supervised setting.
Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the
Recurrent Neural Network Grammar [8].
In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences.
7 Conclusion
In this work, we presented the Transformer, the first sequence transduction model based entirely on
attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with
multi-headed self-attention.
For translation tasks, the Transformer can be trained significantly faster than architectures based
on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014
English-to-French translation tasks, we achieve a new state of the art. In the former task our best
model outperforms even all previously reported ensembles.
We are excited about the future of attention-based models and plan to apply them to other tasks. We
plan to extend the Transformer to problems involving input and output modalities other than text and
to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs
such as images, audio and video. Making generation less sequential is another research goals of ours.
The code we used to train and evaluate our models is available at https://github.com/
tensorflow/tensor2tensor.
Acknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful
comments, corrections and inspiration.
References
[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint
arXiv:1607.06450, 2016.
[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly
learning to align and translate. CoRR, abs/1409.0473, 2014.
[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural
machine translation architectures. CoRR, abs/1703.03906, 2017.
[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine
reading. arXiv preprint arXiv:1601.06733, 2016.
10
[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,
and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical
machine translation. CoRR, abs/1406.1078, 2014.
[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv
preprint arXiv:1610.02357, 2016.
[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation
of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.
[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural
network grammars. In Proc. of NAACL, 2016.
[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.
[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint
arXiv:1308.0850, 2013.
[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition, pages 770–778, 2016.
[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in
recurrent nets: the difficulty of learning long-term dependencies, 2001.
[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,
9(8):1735–1780, 1997.
[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations
across languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural
Language Processing, pages 832–841. ACL, August 2009.
[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring
the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.
[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural
Information Processing Systems, (NIPS), 2016.
[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference
on Learning Representations (ICLR), 2016.
[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,
2017.
[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.
In International Conference on Learning Representations, 2017.
[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.
[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint
arXiv:1703.10722, 2017.
[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen
Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint
arXiv:1703.03130, 2017.
[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task
sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.
[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attentionbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.
11
[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.
[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In
Proceedings of the Human Language Technology Conference of the NAACL, Main Conference,
pages 152–159. ACL, June 2006.
[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention
model. In Empirical Methods in Natural Language Processing, 2016.
[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive
summarization. arXiv preprint arXiv:1705.04304, 2017.
[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,
and interpretable tree annotation. In Proceedings of the 21st International Conference on
Computational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July
2006.
[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv
preprint arXiv:1608.05859, 2016.
[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words
with subword units. arXiv preprint arXiv:1508.07909, 2015.
[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,
and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts
layer. arXiv preprint arXiv:1701.06538, 2017.
[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine
Learning Research, 15(1):1929–1958, 2014.
[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory
networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,
Advances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,
Inc., 2015.
[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural
networks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.
[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.
Rethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.
[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In
Advances in Neural Information Processing Systems, 2015.
[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang
Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine
translation system: Bridging the gap between human and machine translation. arXiv preprint
arXiv:1609.08144, 2016.
[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with
fast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.
[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate
shift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume
1: Long Papers), pages 434–443. ACL, August 2013.
12
Attention Visualizations
Input-Input Layer5 Itisinthis spirit that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
It
is
in
this
spirit
that
a
majority
of
American
governments
have
passed
new
laws
since
2009
making
the
registration
or
voting
process
more
difficult
.
<EOS>
<pad>
<pad>
<pad>
<pad>
<pad>
<pad>
Figure 3: An example of the attention mechanism following long-distance dependencies in the
encoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of
the verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for
the word ‘making’. Different colors represent different heads. Best viewed in color.
13
Input-Input Layer5 The Law will never beperfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5 The Law will never beperfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:
Full attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5
and 6. Note that the attentions are very sharp for this word.
14
Input-Input Layer5 The Law will never beperfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Input-Input Layer5 The Law will never beperfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
The
Law
will
never
be
perfect
,
but
its
application
should
be
just
-
this
is
what
we
are
missing
,
in
my
opinion
.
<EOS>
<pad>
Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the
sentence. We give two such examples above, from two different heads from the encoder self-attention
at layer 5 of 6. The heads clearly learned to perform different tasks.
15
Transformer: A Novel Neural Network Architecture for Language Understanding
THURSDAY, AUGUST 31, 2017
Posted by Jakob Uszkoreit, Software Engineer, Natural Language Understanding

Neural networks, in particular recurrent neural networks (RNNs), are now at the core of the leading approaches to language understanding tasks such as language modeling, machine translation and question answering. In “Attention Is All You Need”, we introduce the Transformer, a novel neural network architecture based on a self-attention mechanism that we believe to be particularly well suited for language understanding.

In our paper, we show that the Transformer outperforms both recurrent and convolutional models on academic English to German and English to French translation benchmarks. On top of higher translation quality, the Transformer requires less computation to train and is a much better fit for modern machine learning hardware, speeding up training by up to an order of magnitude.


BLEU scores (higher is better) of single models on the standard WMT newstest2014 English to German translation benchmark.

BLEU scores (higher is better) of single models on the standard WMT newstest2014 English to French translation benchmark.
Accuracy and Efficiency in Language Understanding
Neural networks usually process language by generating fixed- or variable-length vector-space representations. After starting with representations of individual words or even pieces of words, they aggregate information from surrounding words to determine the meaning of a given bit of language in context. For example, deciding on the most likely meaning and appropriate representation of the word “bank” in the sentence “I arrived at the bank after crossing the…” requires knowing if the sentence ends in “... road.” or “... river.”

RNNs have in recent years become the typical network architecture for translation, processing language sequentially in a left-to-right or right-to-left fashion. Reading one word at a time, this forces RNNs to perform multiple steps to make decisions that depend on words far away from each other. Processing the example above, an RNN could only determine that “bank” is likely to refer to the bank of a river after reading each word between “bank” and “river” step by step. Prior research has shown that, roughly speaking, the more such steps decisions require, the harder it is for a recurrent network to learn how to make those decisions.

The sequential nature of RNNs also makes it more difficult to fully take advantage of modern fast computing devices such as TPUs and GPUs, which excel at parallel and not sequential processing. Convolutional neural networks (CNNs) are much less sequential than RNNs, but in CNN architectures like ByteNet or ConvS2S the number of steps required to combine information from distant parts of the input still grows with increasing distance.


The Transformer
In contrast, the Transformer only performs a small, constant number of steps (chosen empirically). In each step, it applies a self-attention mechanism which directly models relationships between all words in a sentence, regardless of their respective position. In the earlier example “I arrived at the bank after crossing the river”, to determine that the word “bank” refers to the shore of a river and not a financial institution, the Transformer can learn to immediately attend to the word “river” and make this decision in a single step. In fact, in our English-French translation model we observe exactly this behavior.

More specifically, to compute the next representation for a given word - “bank” for example - the Transformer compares it to every other word in the sentence. The result of these comparisons is an attention score for every other word in the sentence. These attention scores determine how much each of the other words should contribute to the next representation of “bank”. In the example, the disambiguating “river” could receive a high attention score when computing a new representation for “bank”. The attention scores are then used as weights for a weighted average of all words’ representations which is fed into a fully-connected network to generate a new representation for “bank”, reflecting that the sentence is talking about a river bank.

The animation below illustrates how we apply the Transformer to machine translation. Neural networks for machine translation typically contain an encoder reading the input sentence and generating a representation of it. A decoder then generates the output sentence word by word while consulting the representation generated by the encoder. The Transformer starts by generating initial representations, or embeddings, for each word. These are represented by the unfilled circles. Then, using self-attention, it aggregates information from all of the other words, generating a new representation per word informed by the entire context, represented by the filled balls. This step is then repeated multiple times in parallel for all words, successively generating new representations.


The decoder operates similarly, but generates one word at a time, from left to right. It attends not only to the other previously generated words, but also to the final representations generated by the encoder.


Flow of Information
Beyond computational performance and higher accuracy, another intriguing aspect of the Transformer is that we can visualize what other parts of a sentence the network attends to when processing or translating a given word, thus gaining insights into how information travels through the network.

To illustrate this, we chose an example involving a phenomenon that is notoriously challenging for machine translation systems: coreference resolution. Consider the following sentences and their French translations:


It is obvious to most that in the first sentence pair “it” refers to the animal, and in the second to the street. When translating these sentences to French or German, the translation for “it” depends on the gender of the noun it refers to - and in French “animal” and “street” have different genders. In contrast to the current Google Translate model, the Transformer translates both of these sentences to French correctly. Visualizing what words the encoder attended to when computing the final representation for the word “it” sheds some light on how the network made the decision. In one of its steps, the Transformer clearly identified the two nouns “it” could refer to and the respective amount of attention reflects its choice in the different contexts.


The encoder self-attention distribution for the word “it” from the 5th to the 6th layer of a Transformer trained on English to French translation (one of eight attention heads).
Given this insight, it might not be that surprising that the Transformer also performs very well on the classic language analysis task of syntactic constituency parsing, a task the natural language processing community has attacked with highly specialized systems for decades.

In fact, with little adaptation, the same network we used for English to German translation outperformed all but one of the previously proposed approaches to constituency parsing.


Next Steps
We are very excited about the future potential of the Transformer and have already started applying it to other problems involving not only natural language but also very different inputs and outputs, such as images and video. Our ongoing experiments are accelerated immensely by the Tensor2Tensor library, which we recently open sourced. In fact, after downloading the library you can train your own Transformer networks for translation and parsing by invoking just a few commands. We hope you’ll give it a try, and look forward to seeing what the community can do with the Transformer.


Acknowledgements
This research was conducted by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser and Illia Polosukhin. Additional thanks go to David Chenell for creating the animation above.

Jay Alammar
Visualizing machine learning one concept at a time.
@JayAlammar on Twitter. YouTube Channel

Blog About
The Illustrated Transformer
Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments)
Translations: Arabic, Chinese (Simplified) 1, Chinese (Simplified) 2, French 1, French 2, Italian, Japanese, Korean, Persian, Russian, Spanish 1, Spanish 2, Vietnamese
Watch: MIT’s Deep Learning State of the Art lecture referencing this post

In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions.

The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter.

2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic:


A High-Level Look
Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.


Popping open that Optimus Prime goodness, we see an encoding component, a decoding component, and connections between them.


The encoding component is a stack of encoders (the paper stacks six of them on top of each other – there’s nothing magical about the number six, one can definitely experiment with other arrangements). The decoding component is a stack of decoders of the same number.


The encoders are all identical in structure (yet they do not share weights). Each one is broken down into two sub-layers:


The encoder’s inputs first flow through a self-attention layer – a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. We’ll look closer at self-attention later in the post.

The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position.

The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence (similar what attention does in seq2seq models).


Bringing The Tensors Into The Picture
Now that we’ve seen the major components of the model, let’s start to look at the various vectors/tensors and how they flow between these components to turn the input of a trained model into an output.

As is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm.




Each word is embedded into a vector of size 512. We'll represent those vectors with these simple boxes.
The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 – In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that’s directly below. The size of this list is hyperparameter we can set – basically it would be the length of the longest sentence in our training dataset.

After embedding the words in our input sequence, each of them flows through each of the two layers of the encoder.


Here we begin to see one key property of the Transformer, which is that the word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.

Next, we’ll switch up the example to a shorter sentence and we’ll look at what happens in each sub-layer of the encoder.

Now We’re Encoding!
As we’ve mentioned already, an encoder receives a list of vectors as input. It processes this list by passing these vectors into a ‘self-attention’ layer, then into a feed-forward neural network, then sends out the output upwards to the next encoder.


The word at each position passes through a self-attention process. Then, they each pass through a feed-forward neural network -- the exact same network with each vector flowing through it separately.
Self-Attention at a High Level
Don’t be fooled by me throwing around the word “self-attention” like it’s a concept everyone should be familiar with. I had personally never came across the concept until reading the Attention is All You Need paper. Let us distill how it works.

Say the following sentence is an input sentence we want to translate:

”The animal didn't cross the street because it was too tired”

What does “it” in this sentence refer to? Is it referring to the street or to the animal? It’s a simple question to a human, but not as simple to an algorithm.

When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.

As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.

If you’re familiar with RNNs, think of how maintaining a hidden state allows an RNN to incorporate its representation of previous words/vectors it has processed with the current one it’s processing. Self-attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing.


As we are encoding the word "it" in encoder #5 (the top encoder in the stack), part of the attention mechanism was focusing on "The Animal", and baked a part of its representation into the encoding of "it".
Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.

Self-Attention in Detail
Let’s first look at how to calculate self-attention using vectors, then proceed to look at how it’s actually implemented – using matrices.

The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.

Notice that these new vectors are smaller in dimension than the embedding vector. Their dimensionality is 64, while the embedding and encoder input/output vectors have dimensionality of 512. They don’t HAVE to be smaller, this is an architecture choice to make the computation of multiheaded attention (mostly) constant.




Multiplying x1 by the WQ weight matrix produces q1, the "query" vector associated with that word. We end up creating a "query", a "key", and a "value" projection of each word in the input sentence.



What are the “query”, “key”, and “value” vectors?

They’re abstractions that are useful for calculating and thinking about attention. Once you proceed with reading how attention is calculated below, you’ll know pretty much all you need to know about the role each of these vectors plays.

The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.

The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.






The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.




This softmax score determines how much each word will be expressed at this position. Clearly the word at this position will have the highest softmax score, but sometimes it’s useful to attend to another word that is relevant to the current word.



The fifth step is to multiply each value vector by the softmax score (in preparation to sum them up). The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).

The sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).




That concludes the self-attention calculation. The resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, however, this calculation is done in matrix form for faster processing. So let’s look at that now that we’ve seen the intuition of the calculation on the word level.

Matrix Calculation of Self-Attention
The first step is to calculate the Query, Key, and Value matrices. We do that by packing our embeddings into a matrix X, and multiplying it by the weight matrices we’ve trained (WQ, WK, WV).


Every row in the X matrix corresponds to a word in the input sentence. We again see the difference in size of the embedding vector (512, or 4 boxes in the figure), and the q/k/v vectors (64, or 3 boxes in the figure)


Finally, since we’re dealing with matrices, we can condense steps two through six in one formula to calculate the outputs of the self-attention layer.


The self-attention calculation in matrix form




The Beast With Many Heads
The paper further refined the self-attention layer by adding a mechanism called “multi-headed” attention. This improves the performance of the attention layer in two ways:

It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.

It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.


With multi-headed attention, we maintain separate Q/K/V weight matrices for each head resulting in different Q/K/V matrices. As we did before, we multiply X by the WQ/WK/WV matrices to produce Q/K/V matrices.

If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices




This leaves us with a bit of a challenge. The feed-forward layer is not expecting eight matrices – it’s expecting a single matrix (a vector for each word). So we need a way to condense these eight down into a single matrix.

How do we do that? We concat the matrices then multiply them by an additional weights matrix WO.


That’s pretty much all there is to multi-headed self-attention. It’s quite a handful of matrices, I realize. Let me try to put them all in one visual so we can look at them in one place






Now that we have touched upon attention heads, let’s revisit our example from before to see where the different attention heads are focusing as we encode the word “it” in our example sentence:


As we encode the word "it", one attention head is focusing most on "the animal", while another is focusing on "tired" -- in a sense, the model's representation of the word "it" bakes in some of the representation of both "animal" and "tired".


If we add all the attention heads to the picture, however, things can be harder to interpret:


Representing The Order of The Sequence Using Positional Encoding
One thing that’s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence.

To address this, the transformer adds a vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word, or the distance between different words in the sequence. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they’re projected into Q/K/V vectors and during dot-product attention.




To give the model a sense of the order of the words, we add positional encoding vectors -- the values of which follow a specific pattern.


If we assumed the embedding has a dimensionality of 4, the actual positional encodings would look like this:


A real example of positional encoding with a toy embedding size of 4


What might this pattern look like?

In the following figure, each row corresponds to a positional encoding of a vector. So the first row would be the vector we’d add to the embedding of the first word in an input sequence. Each row contains 512 values – each with a value between 1 and -1. We’ve color-coded them so the pattern is visible.


A real example of positional encoding for 20 words (rows) with an embedding size of 512 (columns). You can see that it appears split in half down the center. That's because the values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.
The formula for positional encoding is described in the paper (section 3.5). You can see the code for generating positional encodings in get_timing_signal_1d(). This is not the only possible method for positional encoding. It, however, gives the advantage of being able to scale to unseen lengths of sequences (e.g. if our trained model is asked to translate a sentence longer than any of those in our training set).

July 2020 Update: The positional encoding shown above is from the Tensor2Tensor implementation of the Transformer. The method shown in the paper is slightly different in that it doesn’t directly concatenate, but interweaves the two signals. The following figure shows what that looks like. Here’s the code to generate it:


The Residuals
One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.


If we’re to visualize the vectors and the layer-norm operation associated with self attention, it would look like this:


This goes for the sub-layers of the decoder as well. If we’re to think of a Transformer of 2 stacked encoders and decoders, it would look something like this:


The Decoder Side
Now that we’ve covered most of the concepts on the encoder side, we basically know how the components of decoders work as well. But let’s take a look at how they work together.

The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:


After finishing the encoding phase, we begin the decoding phase. Each step in the decoding phase outputs an element from the output sequence (the English translation sentence in this case).
The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.


The self attention layers in the decoder operate in a slightly different way than the one in the encoder:

In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.

The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.

The Final Linear and Softmax Layer
The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.

The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.

Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.

The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.




This figure starts from the bottom with the vector produced as the output of the decoder stack. It is then turned into an output word.


Recap Of Training
Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.

During training, an untrained model would go through the exact same forward pass. But since we are training it on a labeled training dataset, we can compare its output with the actual correct output.

To visualize this, let’s assume our output vocabulary only contains six words(“a”, “am”, “i”, “thanks”, “student”, and “<eos>” (short for ‘end of sentence’)).


The output vocabulary of our model is created in the preprocessing phase before we even begin training.
Once we define our output vocabulary, we can use a vector of the same width to indicate each word in our vocabulary. This also known as one-hot encoding. So for example, we can indicate the word “am” using the following vector:


Example: one-hot encoding of our output vocabulary
Following this recap, let’s discuss the model’s loss function – the metric we are optimizing during the training phase to lead up to a trained and hopefully amazingly accurate model.

The Loss Function
Say we are training our model. Say it’s our first step in the training phase, and we’re training it on a simple example – translating “merci” into “thanks”.

What this means, is that we want the output to be a probability distribution indicating the word “thanks”. But since this model is not yet trained, that’s unlikely to happen just yet.


Since the model's parameters (weights) are all initialized randomly, the (untrained) model produces a probability distribution with arbitrary values for each cell/word. We can compare it with the actual output, then tweak all the model's weights using backpropagation to make the output closer to the desired output.


How do you compare two probability distributions? We simply subtract one from the other. For more details, look at cross-entropy and Kullback–Leibler divergence.

But note that this is an oversimplified example. More realistically, we’ll use a sentence longer than one word. For example – input: “je suis étudiant” and expected output: “i am a student”. What this really means, is that we want our model to successively output probability distributions where:

Each probability distribution is represented by a vector of width vocab_size (6 in our toy example, but more realistically a number like 30,000 or 50,000)
The first probability distribution has the highest probability at the cell associated with the word “i”
The second probability distribution has the highest probability at the cell associated with the word “am”
And so on, until the fifth output distribution indicates ‘<end of sentence>’ symbol, which also has a cell associated with it from the 10,000 element vocabulary.

The targeted probability distributions we'll train our model against in the training example for one sample sentence.


After training the model for enough time on a large enough dataset, we would hope the produced probability distributions would look like this:


Hopefully upon training, the model would output the right translation we expect. Of course it's no real indication if this phrase was part of the training dataset (see: cross validation). Notice that every position gets a little bit of probability even if it's unlikely to be the output of that time step -- that's a very useful property of softmax which helps the training process.
Now, because the model produces the outputs one at a time, we can assume that the model is selecting the word with the highest probability from that probability distribution and throwing away the rest. That’s one way to do it (called greedy decoding). Another way to do it would be to hold on to, say, the top two words (say, ‘I’ and ‘a’ for example), then in the next step, run the model twice: once assuming the first output position was the word ‘I’, and another time assuming the first output position was the word ‘a’, and whichever version produced less error considering both positions #1 and #2 is kept. We repeat this for positions #2 and #3…etc. This method is called “beam search”, where in our example, beam_size was two (meaning that at all times, two partial hypotheses (unfinished translations) are kept in memory), and top_beams is also two (meaning we’ll return two translations). These are both hyperparameters that you can experiment with.

Go Forth And Transform
I hope you’ve found this a useful place to start to break the ice with the major concepts of the Transformer. If you want to go deeper, I’d suggest these next steps:

Read the Attention Is All You Need paper, the Transformer blog post (Transformer: A Novel Neural Network Architecture for Language Understanding), and the Tensor2Tensor announcement.
Watch Łukasz Kaiser’s talk walking through the model and its details
Play with the Jupyter Notebook provided as part of the Tensor2Tensor repo
Explore the Tensor2Tensor repo.
Follow-up works:

Depthwise Separable Convolutions for Neural Machine Translation
One Model To Learn Them All
Discrete Autoencoders for Sequence Models
Generating Wikipedia by Summarizing Long Sequences
Image Transformer
Training Tips for the Transformer Model
Self-Attention with Relative Position Representations
Fast Decoding in Sequence Models using Discrete Latent Variables
Adafactor: Adaptive Learning Rates with Sublinear Memory Cost
Acknowledgements
Thanks to Illia Polosukhin, Jakob Uszkoreit, Llion Jones , Lukasz Kaiser, Niki Parmar, and Noam Shazeer for providing feedback on earlier versions of this post.

Please hit me up on Twitter for any corrections or feedback.

Written on June 27, 2018
Subscribe to get notified about upcoming posts by email
Email Address
Creative Commons License
This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.
Attribution example:
Alammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated-transformer/

Note: If you translate any of the posts, let me know so I can link your translation to the original post. My email is in the about page.
  
Ask AI to edit or generate...

Introduction
0:00
The thing I would very much like to talk about today is the state of the art in deep learning.
0:06
Here we stand in 2019 really at the height of some of the great accomplishments
0:11
that have happened. But also stand at the beginning. And it's up to us to define where this incredible
0:18
data-driven technology takes us. And so I'd like to talk a little bit about the breakthroughs that happened in 2017 and 2018
0:26
that take us to this point. So this lecture is not on the state of the art results on
0:36
main machine learning benchmarks. So the various image classification and object detection
0:43
or the NLP benchmarks or the GAN benchmarks. This isn't about the cutting edge algorithm
0:51
that's available on github that performs best on a particular benchmark. This is about ideas
0:58
ideas and developments that are at the cutting edge of what defines this exciting field of deep learning.
1:05
And so I'd like to go through a bunch of different areas that I think they're really exciting.
1:11
Of course this is also not a lecture that's complete There's other things that may be totally missing that happened
1:18
in 2017-18 that are particularly exciting to people here and people beyond.
1:24
For example medical applications of deep learning is something I totally don't touch on.
1:30
And protein folding and all kinds of applications that there has been some exciting developments
1:36
from deep mind and so on that don't touch on. So forgive me if your favorite developments are missing
1:43
but hopefully this encompasses some of the really fundamental things that have happened
1:48
both on the theory side and the application side and then the community side of all of us being able to work
1:54
together on this and these kinds of technologies. I think 2018 in terms of deep learning is the year of
BERT and Natural Language Processing
2:01
natural language processing. Many have described this year as the ImageNet moment.
2:08
In 2012 for computer vision when AlexNet was the first neural network that really gave that big jump in performance.
2:16
And computer vision it started to inspire people what's possible with deep learning with purely learning based methods.
2:22
In the same way there's been a series of developments from 2016-17 led up to 18 with a development of BERT
2:33
that has made on benchmarks and in our ability to
2:40
apply NLP to solve various NLP tasks, natural language processing tasks a total leap.
2:47
So let's tell the story of what takes us there. There's a few developments. I've mentioned a little bit on Monday
2:54
about the encoder decoder or recurrent neural networks. So this idea of recurrent neural networks encode sequences of data
3:06
and output something, output either a single prediction or another sequence.
3:13
When the input sequence and the output sequence are not the same, necessarily the same size,
3:19
they're like in machine translation we have to translate from one language to another the encoder decoder architecture takes the following process.
3:30
It takes in the sequence of words or the sequence of samples as the input
3:35
and uses the recurrent units whether LSTM, GRU and beyond
3:42
and encodes that sentence into a single vector. So forms an embedding of that sentence of what it
3:51
represent, representation of that sentence. And then feeds that representation in the decoder
3:59
recurrent neural network that then generates the sequence of words that form
4:08
the sentence in the language that's being translated to. So first you encode by taking the sequence and
4:16
mapping it to a fixed size vector representation. And then you decode by taking that fixed size vector representation
4:25
and unrolling it into the sentence that can be of different length than the input sentence. Okay that's the encoder-decoder structure for recurrent neural networks
4:33
has been very effective for machine translation and dealing with arbitrary length input sequences,
4:40
arbitrary length output sequences. Next step attention.
4:45
What is attention? Well it's the next step beyond it's an improvement on the
4:50
the encoder-decoder architecture.
4:56
It allows the, it provides a mechanism that allows to look back at the input sequence.
5:02
So suppose to saying that you have a sequence that's the input sentence
5:09
and that all gets collapsed into a single vector representation. You're allowed to look back at the particular samples from the input sequence
5:17
as part of the decoding process. That's attention and you can also learn which aspects
5:24
are important for which aspects of the decoding process, which aspects the input sequence
5:31
are important to the output sequence. Visualize in another way
5:36
and there's a few visualizations here. They're quite incredible that are done by Jay Alammar.
5:44
I highly recommend you follow the links and look at the
5:51
further details of these visualizations of attention. So if we look at neural machine translation
5:56
the encoder RNN takes a sequence of words and throughout, after every sequence forms a set of
6:06
hidden representations, hidden state that captures the representation of the worlds that followed.
6:12
And those sets of hidden representations as opposed to being collapsed to a single fixed size vector, are then
6:19
all pushed forward to the decoder. That are then used by the decoder to translate
6:25
but in a selective way. Where the decoder here visualized on the y-axis
6:32
the input language and on the X the output language the decoder weighs the different parts of the input sequence differently
6:43
in order to determine how to best translate generate the word that forms a translation in the full output sentence.
6:51
Okay that's attention, allowing expanding the encoder-decoder architecture
6:57
to allow for selective attention to the input sequence
7:05
as opposed to collapsing everything down into fixed representation. Okay next step self-attention.
7:12
In the encoding process allowing the encoder to also
7:19
selectively look informing the hidden representations
7:24
at other parts of the input sequence in order to form those representations.
7:29
It allows you to determine for certain words.
7:35
What are the important relevant aspects of the input sequence that can help you encode that word the best?
7:43
So it improves the encoder process by allowing it to look at the entirety of the context. That's self-attention.
7:52
Building a transformer. It's using the self attention mechanism in the encoder
8:00
to form these sets of representations on the input sequence. And then as part of the decoding process follow the same
8:08
but in reverse with a bunch of self-attention that's able to look back again.
8:13
So it's self attention on the encoder attention on the decoder and that's where the magic, that's where the entirety magic is.
8:21
That's able to capture the rich context of the input sequence in order to generate
8:27
in the contextual way the output sequence. So let's take a step back then and look at what is critical to natural language
8:36
in order to be able to reason about words, construct a language model
8:42
and be able to reason about the words in order to classify a sentence or translate a sentence
8:48
or compare two sentences and so on. There the sentences are collections of words or characters
8:57
and those characters and words have to have an efficient representation that's meaningful for that kind of understanding.
9:03
And that's what the process of embedding is. We talked a little bit about it on Monday. And so the traditional Word2Vec  process of
9:11
embedding is you use some kind of trick in an unsupervised way to map words into
9:17
into a compressed representation. So language modeling is the process of determining
9:26
which words follow each other usually. So one way you can use it as in a skip gram model
9:33
taking a huge datasets of words you know, there's writing all over the place taking those datasets
9:39
and feeding a neural network that in a supervised way looks
9:45
at which words are usually follow the input. So the input is a word the output is which word are
9:53
statistically likely to follow that word. And the same with the preceding word. And doing this kind of unsupervised learning
10:02
if you throw away the output and the input and just taking the hidden representation form in the middle
10:08
that's how you form this compressed embedding a meaningful representation that when
10:14
two words are related in a language modeling sense, two words that are related they're going to be in that representation close to each other.
10:21
And when they're totally unrelated have nothing to do with each other they're far away ELMo is the approach of using bi-directional L STMs
10:33
to learn that representation. And what bi-directional, bi-directionally? So looking not just the sequence that let up to the word but in both directions the sequence that
10:39
following, the sequence that before. And that allows you to learn the rich full context of the word.
10:49
In learning the rich full context of the word you're forming representations that are much better able to represent the statistical language model
11:04
behind the kind of corpus of language that you're you're looking at. And this has taken a big leap in ability to then
11:11
that for further algorithms then with the language model a reasoning about doing things like
11:17
sentence classification, sentence comparison, so on. Translation that representation is much more effective
11:24
for working with language. The idea of the OpenAI transformer
11:30
is the next step forward is taking the the same transformer that I mentioned previously.
11:37
The encoder with self-attention decoder with attention looking back at the input sequence.
11:42
And using, taking the language learned by the decoder
11:52
and using that as a language model and then chopping off layers and training in a specific on a specific language tasks like sentence classification.
12:02
Now BERT is the thing that did the big leap in performance.
12:07
With the transformer formulation there is always there's no bi-directional element.
12:13
There is, it's always moving forward. So the encoding step and the decoding step with BERT is
12:20
it's richly bi-directional it takes in the full sequence of the sentence
12:29
and masks out some percentage of the words, 15% of the words.
12:36
15% of the samples of tokens from the sequence. And tasks the entire encoding
12:46
self-attention mechanism to predict the words that are missing.
12:51
That construct and then you stack a ton of them together. A ton of those encoders self-attention feed-forward network,
13:02
self attention feed forward network together. And that allows you to learn the rich context of the language to then at the end perform all kinds of tasks.
13:10
You can create first of all, like Elmo and like Word2Vec, create rich contextual embeddings.
13:17
Take a set of words and represent them in the space that's very efficient to reason with.
13:24
You can do language classification, you can do settings pair classification,
13:29
you can do the similarity of two sentences, multiple choice question answering, general question answering,
13:34
tagging of sentences. okay I'll link it on that one a little bit too long.
13:42
but it is also the one I'm really excited about and really if there's a breakthrough this year
13:48
is been it's thanks to BERT. The other thing I'm very excited about is totally
13:54
jumping away from the new rips,
Tesla Autopilot Hardware v2+: Neural Networks at Scale
14:00
the theory, those kind of academic developments and deep learning and into the world of applied deep learning.
14:10
So Tesla has a system called Autopilot
14:15
where the hardware version 2 of that system
14:20
is a newer  implementation of the NVIDIA Drive PX 2 system
14:28
which runs a ton of neural networks. There's 8 cameras on the car and
14:36
a variant of the inception network is now taking in all a cameras
14:45
at different resolutions as input and performing various tasks,
14:51
like drivable area segmentation, like object detection and some basic localization tasks.
14:58
So you have now a huge fleet of vehicles where it's not engineers
15:06
some I'm sure engineers but it's really regular consumers, people that have purchased the car have no understanding
15:13
in many cases of what neural networks limitations the capabilities are so on. Now it has a neural network is controlling the well being
15:21
has its decisions, its perceptions and the control decisions based on those perceptions
15:27
are controlling the life of a human being. And that to me is one of the great breakthroughs of 17 and 18.
15:35
In terms of the development of what AI can do in a practical sense in impacting the world.
15:44
And so one billion miles over 1 billion miles have been driven in Autopilot.
15:49
Now there's two types of systems in currently operating in Tesla's. .There's hardware version 1, hardware version 2.
15:56
Hardware version 1 was Intel Mobileye monocular camera perception system.
16:01
As far as we know that was not using a neural network. And it was a fix system. That wasn't learning, at least online learning in the Tesla's.
16:08
The other is hardware version 2 and it's about half and half now in terms of the miles driven.
16:14
The hardware version 2 has a neural network that's always learning. There's weekly updates. It's always improving the model shipping new weights and so on.
16:22
That's the exciting set of breakthroughs in terms of AutoML, the dream of automating some aspects or
AdaNet: AutoML with Ensembles
16:31
all aspects or many aspects as possible of the machine learning process where you can just drop in a dataset that you're working on
16:42
and the system will automatically determine all the parameters
16:47
from the details of the architectures, the size are the architecture, the different modules and then architecture
16:54
the hyper parameters use for training the architecture running that they're doing the inference everything.
17:00
All is done for you. All you just feed it is data So that's been the success of the neural architecture search in 16 and 17.
17:10
And there's been a few ideas with Google AutoML that's really trying to almost create an API we just drop in data set.
17:16
And it's using reinforcement learning and recurrent neural networks to given a few modules,
17:24
stitch them together in such a way where the objective function is optimizing the performance of the overall system.
17:30
And they've showed a lot of exciting results. Google showed and others that outperform state of art systems
17:36
both in terms of efficiency and in terms of accuracy. Now in 18 there've been a few improvements on
17:44
this direction and one of them is a AdaNet where it's now using the same reinforcement
17:51
learning AutoML formulation to build ensembles on your network. So in many cases state-of-the-art performance can be achieved
17:59
by as opposed to taking a single architecture, is building up a multitude and ensemble a collection of architectures.
18:07
And that's what is doing here is given candidate architectures,
18:12
stitching them together to form an ensemble to get state-of-the-art performance. Now that state of the art performance is not a leap
18:20
a breakthrough leap forward but it's nevertheless a step forward. And it's a very exciting field that's going to be
18:28
receiving more and more attention. There's an area of machine learning that's heavily under studied
AutoAugment: Deep RL Data Augmentation
18:35
and I think it's extremely exciting area. And if you look at 2012 with AlexNet achieving
18:46
the breakthrough performance of showing what deep learning networks are capable of.
18:52
From that point, from 2012 to today there's been non-stop
18:58
extremely active developments of different architectures that even on just ImageNet alone on doing the image classification task
19:04
have improved performance over and over and over with totally new ideas.
19:11
Now on the other side on the data side there's been very few ideas about how to do data augmentation.
19:19
So data augmentation is the process of, you know, it's what
19:27
kids always do when you learn about an object right? You look at an object and you kind of like twist it around is
19:36
is taking the raw data and messing it in such a way
19:42
that it can give you much richer representation of what this can this data can look like in other forms
19:48
in other contexts in the real world. There's been very few developments I think still
19:55
and there's this AutoAugment is just a step a tiny step into that direction that I hope that
20:03
we as a community invest a lot of effort in. So what AutoAugment does? As it says, ok, so there's these data augmentation methods
20:13
like translating the image, sharing the image, doing color manipulation like color inversion.
20:19
Let's take those as basic actions you can take and then use reinforcement learning and an RNN again construct to stitch those actions
20:30
together in such a way that can augment data like an ImageNet, you train on the data, it gets state-of-the-art performance.
20:40
So mess with the data in a way that optimizes the way you mess with the data. So.
20:47
And then they've also showed that given that the
20:52
set of data augmentation policies that are learned to optimize for example for ImageNet
21:02
given the some kind of architecture you can take that learn the set of policies for data augmentation and apply it to a totally different dataset.
21:11
So there's the process of transfer learning. So what is transfer learning?
21:17
We talked about transfer learning, you have a neural network that learns to do cat versus dog
21:23
or no learns to do a thousand class classification problem on image. And then you transfer, you chop off few layers and you transfer on the task of
21:30
your own dataset of cat versus dog. What you're transferring is the weights
21:37
that are learned on the ImageNet classification task. And now you're then fine-tuning those weights on the
21:45
specific, personal cat vs. dog dataset you have.
21:52
Now you can do the same thing here. You can transfer as part of the transfer learning process,
21:59
take the data augmentation policies learned on ImageNet,
22:04
and transfer those. You can transfer both the weights and the policies. That's a really super exciting idea I think.
22:13
It wasn't quite demonstrated extremely well here in terms of performance,
22:18
so it got an improvement in performance and so on, but any kind of inspired an idea that's something
22:25
that we need to really think about. How to augment data in an interesting way such that given just a few samples of data?
22:34
We can generate huge data sets in a way that you can then form meaningful complex rich representations from.
22:43
I think that's really exciting in one of the ways that you break open the problem of how do we learn a lot from a little.
22:50
Training deep neural networks with synthetic data. This also really an exciting topic
Training Deep Networks with Synthetic Data
22:59
that a few groups but especially NVIDIA invested a lot in. Here's a from a CVPR2018 probably my favorite work on this topic
23:08
is they really went crazy and said ok let's mess
23:14
with synthetic data in every way we could possibly can. So on the left there're shown a set of backgrounds
23:21
then there's also a set of artificial objects and you have a car or some kind of object that you're trying to classify.
23:27
So let's take that car and mess with it with every way possible. Apply lighting variation to whatever way possible,
23:35
rotate everything that is crazy so what NVIDIA is really good at is creating realistic scenes.
23:43
And they said okay let's create realistic scenes but let's also go away aboveboard and not do realistic at all.
23:50
Do things that can't possibly happen in reality. And so generally these huge datasets I want
23:56
to train and again achieve quite interesting quite a quite good performance
24:03
on image classification. Of course they're trying to apply  to ImageNet and so on these kinds of tasks,
24:08
you're not going to outperform networks that were trained on ImageNet. But they show that with just a small sample from from those real images
24:18
they can fine tune this network train on synthetic images, totally fake images to achieve state of the art performance.
24:24
Again another way to generate, to get, to learn a lot for very little
24:29
by generating fake worlds synthetically.
Segmentation Annotation with Polygon-RNN
24:37
The process of annotation which for supervised learning is what you need to do in order to
24:45
train the network, you need to be able to provide ground truth, you need to be able to label whatever the entity that is being learned.
24:52
And so for image classification that's saying what is going on in the image. And part of that was done on ImageNet by
24:59
doing a Google search for creating candidates. Now saying what's going on in the image is a pretty easy tasks.
25:06
Then there is the object detection task of detecting the boundary box.
25:12
And so saying drawing the actual boundary box is a little bit more difficult but it's a couple of clicks and so on.
25:19
Then if we take the finals the probably one of the higher complexity tasks of perception
25:27
of image understanding is segmentation. It's actually drawing either pixel level or polygons
25:34
the outline of particular object. Now if you have to annotate that that's extremely costly.
25:39
So the work with Polygon-RNN is to use recurrent neural networks to make suggestions for polygons.
25:46
It's really interesting. There's a few tricks to form these high-resolution polygons.
25:52
So the idea is it drops in a single point you draw a boundary box around an object.
25:59
You use convolutional neural networks to drop the first point. And then use recurrent neural networks to draw around it.
26:07
And the performance is really good There's a few tricks and this tool is available online.
26:13
It's a really interesting idea again the dream with AutoML is to remove
26:18
the human from the picture as much as possible. With data augmentation remove the human from the
26:23
picture as much as possible for a menial data. Automate the boring stuff and in this case
26:28
the act of drawing a polygon tried to automated as much as possible. The interesting other dimension along which
DAWNBench: Training Fast and Cheap
26:41
deep learning is recently being trying to be optimized is how do we make deep learning accessible.
26:50
Fast, cheap, accessible. So the DAWNBench from Stanford the benchmark
26:56
the DAWNBench benchmark from Stanford asked formulated an interesting competition,
27:03
which got a lot of attention and a lot of progress. It's saying if we want to achieve 93% accuracy
27:10
on ImageNet and 94% on CIFAR10, let's now compete, that's like the requirement,
27:18
let's now compete how you can do it in the least amount of time and for the least amount of dollars.
27:24
Do the training in the least amount of time and the training in the least amount of dollars like literally dollars you are allowed to spend to do this.
27:33
And fast AI you know it's a renegade awesome renegade group of deep learning researchers
27:41
have been able to train on ImageNet in 3 hours. So this is for training process for 25 bucks.
27:49
So training a network that achieves 93% accuracy for 25 bucks,
27:55
and 94% accuracy for 26 cents on CIFAR10.
28:00
So the key idea that they were playing with is quite simple. But really boils down to messing with the learning rate
28:09
throughout the process of training. So the learning rate is how much you based on the loss function
28:15
based on the error the neural network observes, how much do you adjust the weights. So they found that if they crank up the learning rate
28:27
while decreasing the momentum, which is a parameter of the optimization process,
28:33
and they do it that jointly they're able to make the network learn really fast. That's really exciting and the benchmark itself is also really exciting
28:43
because that's exactly for people sitting in this room that opens up the door to doing all kinds of fundamental deep learning
28:52
problems without the resources of Google DeepMind or OpenAI or Facebook or so on, without computational resources.
29:00
That's important for academia that's important for independent researchers and so on. So GANs. There's been a lot of work on
BigGAN: State of the Art in Image Synthesis
29:09
generative adversarial neural networks. And in some ways there has not been breakthrough
29:16
ideas in GANs for quite a bit.
29:22
And I think began from Google DeepMind an ability to generate
29:29
incredibly high-resolution images. And it's the same GAN technique,
29:36
so in terms of breakthroughs and innovations but scaled. So the increase the model capacity and increase the the batch size
29:44
the number of images that are fed that are fed to the network. It produces incredible images
29:52
I encourage you to go online and and look at them It's hard to believe that they're generated.
29:59
So that was 2018 for GANs was a year of scaling and parameter tuning
30:09
as opposed to breakthrough new ideas. Video-to-Video Synthesis. This work is from NVIDIA
Video-to-Video Synthesis
30:18
is looking at the problem so there's been a lot of work on general going from image to image.
30:25
So from a particular image generating another image. So whether it's colorizing an image or just to traditionally define GANs.
30:36
The idea with video to video synthesis that a few people have been working on but NVIDIA took a good step forward is to make the video
30:49
to make the temporal consistency the temporal dynamics part of the optimization process. So make it look not jumpy.
30:56
So if you look here at the comparison the for this particular.
31:01
So the input is the labels on the top left and the output of the of the NVIDIA approach is on the bottom right.
31:12
See it's temper it's very temporarily consistent. If you look at the image to image mapping that's
31:18
that state the pix2pixHD. It's very jumpy, it's not temporally consistent at all.
31:26
And there's some naive approaches for trying to maintain temporal consistency.
31:31
That's in the bottom left. So you can apply this to all kinds of tasks all kinds of video to video mapping.
31:38
Here is mapping it to face edges. Edge detection on faces mapping it to faces.
31:45
Generating faces from just edges. You can look at body pose to actual images.
31:55
As an input to the network you can take the pose of the person and generate the  video of the person.
32:11
Okay semantic segmentation. The problem of perception, so if began with AlexNet and ImageNet
Semantic Segmentation
32:19
has been further and further developments where the input, the problem is of basic image classification,
32:24
where the input is an image and the output is a classification was going on in that image and the fundamental architecture can be reused
32:31
for more complex tasks like detection like segmentation and so on, interpreting what's going on in the image.
32:37
So these large networks from VGGNet, GoogLeNet, ResNet, SENet, DenseNet
32:45
all these networks are forming rich representations that can then be used for all kinds of tasks whether that task is object detection.
32:52
This here shown is the region based methods where the neural network is tasked the
32:58
convolutional layers make region proposals. So much of candidates to be considered.
33:05
And then there's a step that's determining what's in those different regions and forming boundary boxes around them in a for-loop way.
33:13
And then there is the one-shot method single-shot method where in a single pass
33:18
all of the boundary boxes in their classes generated. And there has been a tremendous amount of work
33:25
in the space of object detection. Some are single shot method, some are region based methods.
33:34
And there's been a lot of exciting work but not more not I would say breakthrough ideas.
33:42
And then we take it to the highest level of perception which is semantic segmentation.
33:48
There's also been a lot of work there the state of the art performance
33:53
is at least for the open source systems is DeepLabv3+ on the PASCAL VOC challenge.
34:03
So semantic segmentation and catch it all up started 2014 with fully convolution neural networks.
34:09
Chopping off the fully connected layers and then outputting the heatmap very grainy very low resolution.
34:21
Then improving that was SegNet performing maxpooling with a breakthrough idea that's reused in a lot of cases is
34:30
Dilated Convolution, Atrous convolutions having some spacing which increases the
34:36
field of view of the convolutional filter. The key idea behind DeepLabv3 that
34:45
is the state of the art is the multi-scale processing.
34:51
Without increasing the parameters the multi scale is achieved by the "atrous rate"
34:58
So taking those atrous convolutions and increasing the spacing. And you can think of the increasing that spacing
35:05
by enlarging the model's field of view. And so you can consider all these different scales of processing and looking at the
35:14
at the layers of features. So allowing you to be able to grasp the greater context
35:24
as part of the upsampling deconvolutional step. And that's what's produced in the state of art performances
35:29
and that's where we have the tutorial on github showing this DeepLab
35:41
architecture trained on CityScapes. CityScapes is a driving segmentation data set
35:50
that is one of the most commonly used for the task of driving scene segmentation.
36:01
Okay on the deep reinforcement learning for.
AlphaZero & OpenAI Five
36:08
So this is touching a bit a bit on the 2017. But i think the excitement really settled in 2018
36:17
as the work from Google and from OpenAI, DeepMind. So it started in DQN paper from Google DeepMind where they beat a bunch of
36:29
a bunch of Atari games achieving superhuman performance with deep reinforcement learning methods.
36:36
That are taking in just the raw pixels of the game, so the same kind of architecture is able to learn how to beat these,
36:42
how to beat these games. Super exciting idea that kind of has echoes
36:47
of what general intelligence is. Taking in the raw raw information and being able to understand
36:54
the game, the sort of physics of the game sufficient to be able to beat it. Then in 2016 AlphaGo with some supervision and some playing against itself,
37:06
self play, some supervised learning on expert world champ players
37:11
and some self play where it plays against itself was able to beat the top of the world champion at Go.
37:19
And then 2017 AlphaGo Zero a specialized version of Alpha Zero
37:25
was able to beat the AlphaGo with just a few days of training.
37:32
and zero supervision from expert games. So through the process of self play again this is kind of
37:39
getting the human out of the picture more and more and more
37:44
which is why Alpha Zero is probably or this AlphaGo Zero was the demonstration of
37:52
the cleanest demonstration of all the nice progress in deep reinforcement learning. I think if we look at the history of AI
37:59
when you're sitting on a porch hundred years from now sort of reminiscing back Alpha Zero will be a thing that people will
38:10
remember as an interesting moment in time, as a key moment in time.
38:15
And Alpha Zero was applied in 2017 to beat.
38:22
Alpha Zero paper was in 2017 and it was this year played StockFish in chess which is the best engine, chess playing engines
38:32
is able to beat it with just four hours of training of course the four hours this caveat.
38:39
Because four hours for Google DeepMind is highly distributed training. So it's not four hours for an undergraduate student sitting in their dorm room.
38:49
But meaning it was able to self play to very quickly learn to beat the state of the art chess engine.
38:56
And learned to beat the state of the art Shogi engine Elmo.
39:02
And the interesting thing here is you know with perfect information games like chess
39:08
you have a tree and you have all the decisions you could possibly make and so the farther along you look at along that tree presumably the better you do.
39:17
That's how DeepBlue beat Kasparov in the 90s is you just look as far as possible in a down the tree
39:26
to determine which is the action is the most optimal. If you look at the way human grandmasters think
39:33
it certainly doesn't feel like they're like looking down a tree. There's something like creative intuition there's something like
39:39
you can see the patterns in the board, you can do a few calculations but really it's an order of hundreds.
39:45
It's not on the order of millions or billions which is kind of the
39:50
the StockFish the state of the art chess engine approach.
39:56
And Alpha Zero is moving closer and closer closer towards the human grandmaster concerning very few future moves.
40:03
It's able through the neural network estimator that's estimating the quality of the move and the quality of the different, the current quality of the board and
40:10
and the quality of the moves that follow. It's able to do much much less look ahead.
40:16
So the neural network learns the fundamental information just like when a grandmaster looks
40:21
at a board they can tell how good that is. So that's again interesting, it's a step towards
40:30
at least echoes of what human intelligence is in this very structured formal constrained world of chess
40:36
and go and shogi. And then there's the other side of the world that's messy.
40:42
It's still games. It's still constrained in that way but OpenAI has taken on the challenge of playing games
40:50
that are much messier to have this resemblance
40:56
of the real world and the fact that you have to do teamwork, you have to look at long time horizons
41:01
with huge amounts of imperfect information, hidden information, uncertainty.
41:07
So within that world they've taken on the challenge of a popular game Dota 2.
41:13
On the human side of that
41:19
there's the competition the international hosted every year where you know in 2018 the winning team gets 11 million dollars. So it's a very popular very active competition has been
41:25
going on for a few years. They've been improving and it achieved a lot of interesting milestones in 2017.
41:35
Their 1v1 bot beat the top professional Dota 2 player. The way you achieve great things is as you try.
41:44
And in 2018 they tried to go 5v5. The OpenAI team lost two games
41:51
a go against the top Dota 2 players at the 2018 international.
41:57
And of course their ranking here the MMR ranking in Dota 2
42:02
has been increasing over and over but there's a lot of challenges here that make it extremely difficult.
42:08
To beat the human players and this is, you know, in every story rocky
42:15
or whatever you think about losing is essential element of a story that leads to then
42:21
a movie in a book and the greatness. So you better believe that they're coming back next year.
42:26
And there's going to be a lot of exciting developments there. It also, Dota 2 and this particular video game makes it currently
42:34
this really two games that have the public eye in terms of AI taking on his benchmarks.
42:42
So we saw go incredible accomplishment What's next? So last year the associate were the best paper in Europe's.
42:52
There was the heads up Texas No Limit Hold'em AI was able to beat the top level players was completely current
43:02
well not completely but currently out of reach is the general not heads up one versus one but the general team
43:09
Texas No Limit Hold'em here you go. And on the gaming side this dream of Dota 2 now
43:17
that's the benchmark that everybody's targeting. And it's actually incredibly difficult one and some people think would be a long time before we can win.
43:24
And on the more practical side of things the
43:30
2018, start in 2017 has been a year of
Deep Learning Frameworks
43:36
of the frameworks growing up of maturing
43:42
and creating ecosystems around them. With TensorFlow with the history there dating back a few years
43:50
has really with TensorFlow 1.0 as come
43:55
to be sort of a mature framework PyTorch 1.0 came out 2018 is matured as well.
44:03
And now the really exciting developments in the TensorFlow with the eager execution and beyond
44:09
that's coming out TensorFlow 2.0 in in 2019. So really those two players have made incredible leaps in standardizing deep learning.
44:24
In the fact that a lot of the ideas I talked about today and Monday and we'll keep talking about
44:30
are all have a github repository with implementations in TensorFlow and PyTorch.
44:36
Making extremely accessible and that's really exciting. it's probably best to quote Geoff Hinton the "Godfather" of deep learning,
2019 and beyond
44:46
one of the key people behind backpropagation said recently on backpropagation is "My view is throw it all away and start again"
44:55
His believes backpropagation is totally broken and an idea that has ancient
45:00
and it needs to be completely revolutionized and the practical protocol for doing that is he said the future
45:08
depends on some graduate student who's deeply suspicious of everything I've said that's probably a good way to end
45:16
the discussion about what the state of the art in deep learning holds because everything we're doing is fundamentally based on
45:25
ideas from the 60s and the 80s and really in terms of
45:30
new ideas, there has not been many new ideas especially the state of the art results that I've mentioned
45:37
are all based on fundamentally, on stochastic gradient descent and backpropagation.
45:45
It's ripe for totally new ideas. So it's up to us to define
45:51
the real breakthroughs and the real state of the art 2019 and beyond. So that I'd like to thank you and
46:00
the stuff is on the website deeplearning.mit.edu.
